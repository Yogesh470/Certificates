# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10U7WNI0w25kiMcoaMoNf3uLOr9LjYeDa
"""

# Install the latest Tensorflow version.
!pip install tensorflow-gpu

#import liberies
#@title Load the Universal Sentence Encoder's TF Hub module
from __future__ import absolute_import, division, print_function, unicode_literals

import numpy 
import pandas as pd

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow import feature_column
from tensorflow.keras import layers
module_url = "https://tfhub.dev/google/universal-sentence-encoder/4" #@param ["https://tfhub.dev/google/universal-sentence-encoder/4", "https://tfhub.dev/google/universal-sentence-encoder-large/5"]
model = hub.load(module_url)
print ("module %s loaded" % module_url)

#The embeddings produced by the Universal Sentence Encoder are approximately normalized. 
#The semantic similarity of two sentences can be trivially computed as the inner product of the encodings

#convert pandas dataframe into tensor using tf.data
df=pd.read_csv('Text_Similarity_Dataset.csv')
features = ['text1','text2']

training_dataset = (
    tf.data.Dataset.from_tensor_slices(
        (
            tf.cast(df[features].values, tf.string)
 
        )
    )
)
for features_tensor in training_dataset:
    print(f'features:{features_tensor} ')

#Evaluate Sentence Embeddings
# using cosine similarity
model = hub.load(module_url)
print ("module %s loaded" % module_url)
def embed(input):
  return model(input)
l=[]  
for i in range(4023):
  m=embed(features_tensor)
  split0, split1 = tf.split(m,2, 0)
  cosine_similarities = tf.reduce_sum(tf.multiply(split0,split1),axis=1)
  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)
  scores = 1.0 - tf.acos(clip_cosine_similarities)
  l.append(scores)
for i in range(len(l)):
  print(l[i])